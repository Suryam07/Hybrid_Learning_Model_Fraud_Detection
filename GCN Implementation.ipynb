{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7P2-SUdydGO",
        "outputId": "a5a39052-6764-4045-c390-07bf183f42d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.13.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga1j7dyFyiab"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hRsZ9WpqkEH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHzIyw4Sqm5W"
      },
      "outputs": [],
      "source": [
        "def run_experiment(model, x_train, y_train):\n",
        "    # Compile the model.\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate),\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")],\n",
        "    )\n",
        "    # Create an early stopping callback.\n",
        "    early_stopping = keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_acc\", patience=10, restore_best_weights=True\n",
        "    )\n",
        "    # Fit the model.\n",
        "    history = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        epochs=num_epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_split=0.15,\n",
        "        callbacks=[early_stopping],\n",
        "    )\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icJZ61_EquoP"
      },
      "outputs": [],
      "source": [
        "def display_learning_curves(history):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    ax1.plot(history.history[\"loss\"])\n",
        "    ax1.plot(history.history[\"val_loss\"])\n",
        "    ax1.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "\n",
        "    ax2.plot(history.history[\"acc\"])\n",
        "    ax2.plot(history.history[\"val_acc\"])\n",
        "    ax2.legend([\"train\", \"test\"], loc=\"upper right\")\n",
        "    ax2.set_xlabel(\"Epochs\")\n",
        "    ax2.set_ylabel(\"Accuracy\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4rhn-g_qwSl"
      },
      "outputs": [],
      "source": [
        "def create_ffn(hidden_units, dropout_rate, name=None):\n",
        "    fnn_layers = []\n",
        "\n",
        "    for units in hidden_units:\n",
        "        fnn_layers.append(layers.BatchNormalization())\n",
        "        fnn_layers.append(layers.Dropout(dropout_rate))\n",
        "        fnn_layers.append(layers.Dense(units, activation=tf.nn.gelu))\n",
        "\n",
        "    return keras.Sequential(fnn_layers, name=name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVfF4hNCqx7g",
        "outputId": "ea0d7d17-d9cc-4040-9387-ab1fced812da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 311: expected 167 fields, saw 169\n",
            "Skipping line 620: expected 167 fields, saw 301\n",
            "Skipping line 1240: expected 167 fields, saw 236\n",
            "Skipping line 1859: expected 167 fields, saw 303\n",
            "Skipping line 2789: expected 167 fields, saw 173\n",
            "Skipping line 3098: expected 167 fields, saw 223\n",
            "Skipping line 3717: expected 167 fields, saw 305\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 4336: expected 167 fields, saw 298\n",
            "Skipping line 4645: expected 167 fields, saw 171\n",
            "Skipping line 5265: expected 167 fields, saw 231\n",
            "Skipping line 6193: expected 167 fields, saw 173\n",
            "Skipping line 6502: expected 167 fields, saw 254\n",
            "Skipping line 6810: expected 167 fields, saw 212\n",
            "Skipping line 7119: expected 167 fields, saw 176\n",
            "Skipping line 8047: expected 167 fields, saw 194\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 8666: expected 167 fields, saw 209\n",
            "Skipping line 9594: expected 167 fields, saw 285\n",
            "Skipping line 10522: expected 167 fields, saw 273\n",
            "Skipping line 12070: expected 167 fields, saw 239\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 12999: expected 167 fields, saw 191\n",
            "Skipping line 13308: expected 167 fields, saw 271\n",
            "Skipping line 13928: expected 167 fields, saw 221\n",
            "Skipping line 14237: expected 167 fields, saw 197\n",
            "Skipping line 14856: expected 167 fields, saw 257\n",
            "Skipping line 15785: expected 167 fields, saw 217\n",
            "Skipping line 16094: expected 167 fields, saw 222\n",
            "Skipping line 16404: expected 167 fields, saw 191\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 17025: expected 167 fields, saw 176\n",
            "Skipping line 17334: expected 167 fields, saw 234\n",
            "Skipping line 17953: expected 167 fields, saw 219\n",
            "Skipping line 18572: expected 167 fields, saw 226\n",
            "Skipping line 19191: expected 167 fields, saw 207\n",
            "Skipping line 20121: expected 167 fields, saw 278\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 20740: expected 167 fields, saw 257\n",
            "Skipping line 21359: expected 167 fields, saw 222\n",
            "Skipping line 21667: expected 167 fields, saw 199\n",
            "Skipping line 22286: expected 167 fields, saw 238\n",
            "Skipping line 22905: expected 167 fields, saw 170\n",
            "Skipping line 23524: expected 167 fields, saw 185\n",
            "Skipping line 23833: expected 167 fields, saw 281\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 24761: expected 167 fields, saw 187\n",
            "Skipping line 25071: expected 167 fields, saw 185\n",
            "Skipping line 25690: expected 167 fields, saw 276\n",
            "Skipping line 26619: expected 167 fields, saw 228\n",
            "Skipping line 27548: expected 167 fields, saw 284\n",
            "Skipping line 28167: expected 167 fields, saw 304\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 29095: expected 167 fields, saw 234\n",
            "Skipping line 30024: expected 167 fields, saw 170\n",
            "Skipping line 30332: expected 167 fields, saw 278\n",
            "Skipping line 30641: expected 167 fields, saw 204\n",
            "Skipping line 31260: expected 167 fields, saw 229\n",
            "Skipping line 32190: expected 167 fields, saw 194\n",
            "Skipping line 32810: expected 167 fields, saw 278\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 33430: expected 167 fields, saw 269\n",
            "Skipping line 34050: expected 167 fields, saw 201\n",
            "Skipping line 34359: expected 167 fields, saw 250\n",
            "Skipping line 34979: expected 167 fields, saw 268\n",
            "Skipping line 35909: expected 167 fields, saw 296\n",
            "Skipping line 36838: expected 167 fields, saw 299\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 37767: expected 167 fields, saw 205\n",
            "Skipping line 38696: expected 167 fields, saw 176\n",
            "Skipping line 39005: expected 167 fields, saw 171\n",
            "Skipping line 39315: expected 167 fields, saw 198\n",
            "Skipping line 39933: expected 167 fields, saw 304\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 41789: expected 167 fields, saw 172\n",
            "Skipping line 42097: expected 167 fields, saw 304\n",
            "Skipping line 42716: expected 167 fields, saw 224\n",
            "Skipping line 43025: expected 167 fields, saw 203\n",
            "Skipping line 43644: expected 167 fields, saw 209\n",
            "Skipping line 44263: expected 167 fields, saw 180\n",
            "Skipping line 44571: expected 167 fields, saw 266\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 45500: expected 167 fields, saw 175\n",
            "Skipping line 46430: expected 167 fields, saw 173\n",
            "Skipping line 46739: expected 167 fields, saw 288\n",
            "Skipping line 47358: expected 167 fields, saw 173\n",
            "Skipping line 48287: expected 167 fields, saw 274\n",
            "Skipping line 49217: expected 167 fields, saw 193\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 49526: expected 167 fields, saw 266\n",
            "Skipping line 49834: expected 167 fields, saw 181\n",
            "Skipping line 50766: expected 167 fields, saw 211\n",
            "Skipping line 51074: expected 167 fields, saw 218\n",
            "Skipping line 51384: expected 167 fields, saw 198\n",
            "Skipping line 52314: expected 167 fields, saw 204\n",
            "Skipping line 52622: expected 167 fields, saw 219\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 53551: expected 167 fields, saw 316\n",
            "Skipping line 54478: expected 167 fields, saw 273\n",
            "Skipping line 55097: expected 167 fields, saw 239\n",
            "Skipping line 56026: expected 167 fields, saw 208\n",
            "Skipping line 56645: expected 167 fields, saw 275\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 57574: expected 167 fields, saw 224\n",
            "Skipping line 58193: expected 167 fields, saw 245\n",
            "Skipping line 58812: expected 167 fields, saw 180\n",
            "Skipping line 59741: expected 167 fields, saw 210\n",
            "Skipping line 60360: expected 167 fields, saw 182\n",
            "Skipping line 60979: expected 167 fields, saw 175\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 62217: expected 167 fields, saw 231\n",
            "Skipping line 62836: expected 167 fields, saw 249\n",
            "Skipping line 63457: expected 167 fields, saw 225\n",
            "Skipping line 63766: expected 167 fields, saw 195\n",
            "Skipping line 65628: expected 167 fields, saw 312\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 66558: expected 167 fields, saw 217\n",
            "Skipping line 66868: expected 167 fields, saw 171\n",
            "Skipping line 67488: expected 167 fields, saw 271\n",
            "Skipping line 68728: expected 167 fields, saw 255\n",
            "Skipping line 69347: expected 167 fields, saw 215\n",
            "Skipping line 69656: expected 167 fields, saw 168\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: ParserWarning: Skipping line 70277: expected 167 fields, saw 185\n",
            "Skipping line 70587: expected 167 fields, saw 229\n",
            "Skipping line 71517: expected 167 fields, saw 288\n",
            "Skipping line 72137: expected 167 fields, saw 182\n",
            "Skipping line 72447: expected 167 fields, saw 217\n",
            "Skipping line 72756: expected 167 fields, saw 202\n",
            "\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
            "<ipython-input-9-ae2d269c1e5e>:2: DtypeWarning: Columns (3,8,10,13,19,25,27,37,42,53,55,62,80,98,132) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n"
          ]
        }
      ],
      "source": [
        "\n",
        "edges = pd.read_csv(\"elliptic_txs_edgelist.csv\")\n",
        "features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, on_bad_lines='warn')\n",
        "# or\n",
        "# features = pd.read_csv(\"elliptic_txs_features.csv\", header=None, error_bad_lines=False)\n",
        "classes = pd.read_csv(\"elliptic_txs_classes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHPCI4Ufq00A",
        "outputId": "bda1767f-32f9-4ac8-f1aa-774aaddf23c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(234355, 2)\n",
            "(73884, 167)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(edges.shape)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ct8KxKSrddH"
      },
      "outputs": [],
      "source": [
        "\n",
        "tx_features = [\"tx_feat_\"+str(i) for i in range(2,95)]\n",
        "agg_features = [\"agg_feat_\"+str(i) for i in range(1,73)]\n",
        "features.columns = [\"txId\",\"time_step\"] + tx_features + agg_features\n",
        "\n",
        "#merge features and classes\n",
        "features = pd.merge(features,classes,left_on=\"txId\",right_on=\"txId\",how='left')\n",
        "features['class'] = features['class'].apply(lambda x: '0' if x == \"unknown\" else x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBmsebrbsQMK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# get the features of the known transactions\n",
        "features= features[features['class'] != '0' ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZR5R0ZfsR1z"
      },
      "outputs": [],
      "source": [
        "def check(name):\n",
        "  if(name in unique):\n",
        "    return name\n",
        "  else :\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTtwfsvosTSv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# remove from the edges DF all the entries that mentions the unknown transactions\n",
        "unique = features['txId'].unique()\n",
        "edges[\"txId1\"] = edges[\"txId1\"].apply(lambda name: check(name))\n",
        "edges[\"txId2\"] = edges[\"txId2\"].apply(lambda name: check(name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OHJ-r0esU5D"
      },
      "outputs": [],
      "source": [
        "\n",
        "edges = edges[edges[\"txId1\"] != -1 ]\n",
        "edges = edges[edges[\"txId2\"] != -1 ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQCbkvlLsWOl",
        "outputId": "98f7cbb1-18cc-4b2d-f5ae-caffec3103f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(11064, 2)\n",
            "(15172, 168)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(edges.shape)\n",
        "print(features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm9qx7HqsXmK"
      },
      "outputs": [],
      "source": [
        "class_values = sorted(features[\"class\"].unique())\n",
        "#we create a new index by sorting the tx_ids and assign to it a number\n",
        "features_idx = {name: idx for idx, name in enumerate(sorted(features[\"txId\"].unique()))}\n",
        "\n",
        "# we then apply this new ids to all te data frames\n",
        "# this helps a lot in computing the adjency matrix, having the ids as the index.\n",
        "features[\"txId\"] = features[\"txId\"].apply(lambda name: features_idx[name])\n",
        "edges[\"txId1\"] = edges[\"txId1\"].apply(lambda name: features_idx[name])\n",
        "edges[\"txId2\"] = edges[\"txId2\"].apply(lambda name: features_idx[name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hy0P3QBcsZtU",
        "outputId": "d13b7436-cab4-4689-aa44-727f5fd1b074"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Edges shape: (2, 11064)\n",
            "Nodes shape: (15172, 93)\n",
            "edge weights shape: (11064,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create an edges array (sparse adjacency matrix) of shape [2, num_edges].\n",
        "edges2 = edges[[\"txId1\", \"txId2\"]].to_numpy().T\n",
        "# Create an edge weights array of ones.\n",
        "edge_weights = tf.ones(shape=edges2.shape[1])\n",
        "# Create a node features array of shape [num_nodes, num_features].\n",
        "# IN the next line we select the features to include in the graph\n",
        "# Notice than only tx_features are present!\n",
        "# Convert all columns in tx_features to numeric, coerce errors to NaN\n",
        "for col in tx_features:\n",
        "    features[col] = pd.to_numeric(features[col], errors='coerce')\n",
        "\n",
        "# Replace NaN values with 0\n",
        "features[tx_features] = features[tx_features].fillna(0)\n",
        "node_features = tf.cast(\n",
        "    features.sort_values(\"txId\")[tx_features].to_numpy(), dtype=tf.dtypes.float32\n",
        ")\n",
        "# Create graph info tuple with node_features, edges, and edge_weights.\n",
        "graph_info = (node_features, edges2, edge_weights)\n",
        "\n",
        "print(\"Edges shape:\", edges2.shape)\n",
        "print(\"Nodes shape:\", node_features.shape)\n",
        "print(\"edge weights shape:\", edge_weights.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTKQ2E2csdvB"
      },
      "outputs": [],
      "source": [
        "class GraphConvLayer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        hidden_units,\n",
        "        dropout_rate=0.2,\n",
        "        aggregation_type=\"mean\",\n",
        "        combination_type=\"concat\",\n",
        "        normalize=False,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(GraphConvLayer, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.aggregation_type = aggregation_type\n",
        "        self.combination_type = combination_type\n",
        "        self.normalize = normalize\n",
        "\n",
        "        self.ffn_prepare = create_ffn(hidden_units, dropout_rate)\n",
        "        if self.combination_type == \"gated\":\n",
        "            self.update_fn = layers.GRU(\n",
        "                units=hidden_units,\n",
        "                activation=\"tanh\",\n",
        "                recurrent_activation=\"sigmoid\",\n",
        "                dropout=dropout_rate,\n",
        "                return_state=True,\n",
        "                recurrent_dropout=dropout_rate,\n",
        "            )\n",
        "        else:\n",
        "            self.update_fn = create_ffn(hidden_units, dropout_rate)\n",
        "\n",
        "    def prepare(self, node_repesentations, weights=None):\n",
        "        # node_repesentations shape is [num_edges, embedding_dim].\n",
        "        messages = self.ffn_prepare(node_repesentations)\n",
        "        if weights is not None:\n",
        "            messages = messages * tf.expand_dims(weights, -1)\n",
        "        return messages\n",
        "\n",
        "    def aggregate(self, node_indices, neighbour_messages):\n",
        "        # node_indices shape is [num_edges].\n",
        "        # neighbour_messages shape: [num_edges, representation_dim].\n",
        "        num_nodes = tf.math.reduce_max(node_indices) + 1\n",
        "        if self.aggregation_type == \"sum\":\n",
        "            aggregated_message = tf.math.unsorted_segment_sum(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"mean\":\n",
        "            aggregated_message = tf.math.unsorted_segment_mean(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "        elif self.aggregation_type == \"max\":\n",
        "            aggregated_message = tf.math.unsorted_segment_max(\n",
        "                neighbour_messages, node_indices, num_segments=num_nodes\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid aggregation type: {self.aggregation_type}.\")\n",
        "\n",
        "        return aggregated_message\n",
        "\n",
        "    def update(self, node_repesentations, aggregated_messages): #Fixed indentation here\n",
        "        # node_repesentations shape is [num_nodes, representation_dim].\n",
        "        # aggregated_messages shape is [num_nodes, representation_dim].\n",
        "        if self.combination_type == \"gru\":\n",
        "            # Create a sequence of two elements for the GRU layer.\n",
        "            h = tf.stack([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"concat\":\n",
        "            # Concatenate the node_repesentations and aggregated_messages.\n",
        "            h = tf.concat([node_repesentations, aggregated_messages], axis=1)\n",
        "        elif self.combination_type == \"add\":\n",
        "            # Add node_repesentations and aggregated_messages.\n",
        "            h = node_repesentations + aggregated_messages\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid combination type: {self.combination_type}.\")\n",
        "\n",
        "        # Apply the processing function.\n",
        "        node_embeddings = self.update_fn(h)\n",
        "        if self.combination_type == \"gru\":\n",
        "            node_embeddings = tf.unstack(node_embeddings, axis=1)[-1]\n",
        "\n",
        "        if self.normalize:\n",
        "            node_embeddings = tf.nn.l2_normalize(node_embeddings, axis=-1)\n",
        "        return node_embeddings\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Process the inputs to produce the node_embeddings.\n",
        "\n",
        "        inputs: a tuple of three elements: node_repesentations, edges, edge_weights.\n",
        "        Returns: node_embeddings of shape [num_nodes, representation_dim].\n",
        "        \"\"\"\n",
        "\n",
        "        node_repesentations, edges, edge_weights = inputs\n",
        "        # Get node_indices (source) and neighbour_indices (target) from edges.\n",
        "        node_indices, neighbour_indices = edges[0], edges[1]\n",
        "        # neighbour_repesentations shape is [num_edges, representation_dim].\n",
        "        neighbour_repesentations = tf.gather(node_repesentations, neighbour_indices)\n",
        "\n",
        "        # Prepare the messages of the neighbours.\n",
        "        neighbour_messages = self.prepare(neighbour_repesentations, edge_weights)\n",
        "        # Aggregate the neighbour messages.\n",
        "        aggregated_messages = self.aggregate(node_indices, neighbour_messages)\n",
        "        # Update the node embedding with the neighbour messages.\n",
        "        return self.update(node_repesentations, aggregated_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xDqTcr_uNyy"
      },
      "outputs": [],
      "source": [
        "class GNNNodeClassifier(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        graph_info,\n",
        "        num_classes,\n",
        "        hidden_units,\n",
        "        aggregation_type=\"sum\",\n",
        "        combination_type=\"concat\",\n",
        "        dropout_rate=0.2,\n",
        "        normalize=True,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(GNNNodeClassifier, self).__init__(*args, **kwargs)\n",
        "\n",
        "        # Unpack graph_info to three elements: node_features, edges, and edge_weight.\n",
        "        node_features, edges, edge_weights = graph_info\n",
        "        self.node_features = node_features\n",
        "        self.edges = edges\n",
        "        self.edge_weights = edge_weights\n",
        "        # Set edge_weights to ones if not provided.\n",
        "        if self.edge_weights is None:\n",
        "            self.edge_weights = tf.ones(shape=edges.shape[1])\n",
        "        # Scale edge_weights to sum to 1.\n",
        "        self.edge_weights = self.edge_weights / tf.math.reduce_sum(self.edge_weights)\n",
        "\n",
        "        # Create a process layer.\n",
        "        self.preprocess = create_ffn(hidden_units, dropout_rate, name=\"preprocess\")\n",
        "        # Create the first GraphConv layer.\n",
        "        self.conv1 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv1\",\n",
        "        )\n",
        "        # Create the second GraphConv layer.\n",
        "        self.conv2 = GraphConvLayer(\n",
        "            hidden_units,\n",
        "            dropout_rate,\n",
        "            aggregation_type,\n",
        "            combination_type,\n",
        "            normalize,\n",
        "            name=\"graph_conv2\",\n",
        "        )\n",
        "        # Create a postprocess layer.\n",
        "        self.postprocess = create_ffn(hidden_units, dropout_rate, name=\"postprocess\")\n",
        "        # Create a compute logits layer.\n",
        "        self.compute_logits = layers.Dense(units=num_classes, name=\"logits\")\n",
        "\n",
        "    def call(self, input_node_indices):\n",
        "        # Preprocess the node_features to produce node representations.\n",
        "        x = self.preprocess(self.node_features)\n",
        "        # Apply the first graph conv layer.\n",
        "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x1 + x\n",
        "        # Apply the second graph conv layer.\n",
        "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
        "        # Skip connection.\n",
        "        x = x2 + x\n",
        "        # Postprocess node embedding.\n",
        "        x = self.postprocess(x)\n",
        "        # Fetch node embeddings for the input node_indices.\n",
        "        node_embeddings = tf.gather(x, input_node_indices)\n",
        "        # Compute logits\n",
        "        return self.compute_logits(node_embeddings)\n",
        "\n",
        "    def get_node_embeddings(self):\n",
        "        \"\"\"Returns the node embeddings learned by the GCN.\"\"\"\n",
        "        x = self.preprocess(self.node_features)\n",
        "        x1 = self.conv1((x, self.edges, self.edge_weights))\n",
        "        x = x1 + x\n",
        "        x2 = self.conv2((x, self.edges, self.edge_weights))\n",
        "        x = x2 + x\n",
        "        x = self.postprocess(x)\n",
        "        return x #Fixed indentation here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc5-93jZAAIT"
      },
      "outputs": [],
      "source": [
        "G = nx.from_edgelist(edges[['txId1', 'txId2']].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SELKMFuY3yzx"
      },
      "outputs": [],
      "source": [
        "# Calculate node degrees\n",
        "node_degrees = dict(G.degree())\n",
        "\n",
        "# Add node degrees as a feature to the features DataFrame\n",
        "features['node_degree'] = features['txId'].map(node_degrees)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt0NJ8W6w_by"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "import sklearn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split ,GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouHp3EJeziKy"
      },
      "outputs": [],
      "source": [
        "# 2. GCN Architecture Modifications (Example: Increase hidden units)\n",
        "gnn_model = GNNNodeClassifier(\n",
        "    graph_info,\n",
        "    num_classes=len(class_values),\n",
        "    hidden_units=[128, 64],  # Increased hidden units\n",
        "    aggregation_type='mean',\n",
        "    combination_type='concat',\n",
        "    dropout_rate=0.2,\n",
        "    normalize=True\n",
        ")\n",
        "\n",
        "node_embeddings = gnn_model.get_node_embeddings().numpy()\n",
        "\n",
        "# 3. Prepare Data for XGBoost\n",
        "X = node_embeddings\n",
        "y = features['class'].astype(int) - 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d93hSl9xGz6"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Hyperparameter Tuning for XGBoost (using GridSearchCV)\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    # ... other XGBoost hyperparameters ...\n",
        "}\n",
        "\n",
        "# Initialize and train XGBoost classifier\n",
        "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(class_values))\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_xgb_model = grid_search.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i0Y0_vUznV6"
      },
      "outputs": [],
      "source": [
        "# 6. Evaluate the best model\n",
        "y_pred = best_xgb_model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIzLvUVdzp4N",
        "outputId": "a843ef2b-9333-4e83-eaf1-e549d2ede8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9127\n",
            "Accuracy Percentage: 91.27%\n",
            "Precision: 0.8330\n",
            "Recall: 0.9127\n",
            "F1 Score: 0.8710\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "# 7. Calculate and Print Metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "accuracy_percentage = accuracy * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Accuracy Percentage: {accuracy_percentage:.2f}%\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwMKd5gx0aVE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
